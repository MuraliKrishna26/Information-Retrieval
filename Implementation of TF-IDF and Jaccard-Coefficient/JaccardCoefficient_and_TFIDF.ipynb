{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Lakshmi\n",
      "[nltk_data]     Praffulla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Lemmatizations\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inside_folder = 'stories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Lakshmi Praffulla\\\\Desktop\\\\Information Retrival Assignment 2\\\\Dataset_for_question1\\\\stories',\n",
       " 'C:\\\\Users\\\\Lakshmi Praffulla\\\\Desktop\\\\Information Retrival Assignment 2\\\\Dataset_for_question1\\\\stories\\\\FARNON',\n",
       " 'C:\\\\Users\\\\Lakshmi Praffulla\\\\Desktop\\\\Information Retrival Assignment 2\\\\Dataset_for_question1\\\\stories\\\\SRE']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = [x[0] for x in os.walk(str(os.getcwd())+'\\\\'+'Dataset_for_question1'+'\\\\'+inside_folder+'\\\\')]\n",
    "#Above command will give the path for directory[0] with \\\\ appended . So we remove \\\\ in next line\n",
    "directory[0] = directory[0][:len(directory[0])-1]\n",
    "\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452 452\n",
      "0 0\n",
      "15 15\n",
      "Total Number of files are: 467\n"
     ]
    }
   ],
   "source": [
    "#Extracting the title names from the data_files\n",
    "\n",
    "\n",
    "data = []\n",
    "titles = []\n",
    "file_name = []\n",
    "check = False\n",
    "\n",
    "for item in directory:\n",
    "    file = open(item+\"\\\\index.html\",\"r\")\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    \n",
    "    nameofFile = re.findall('><A HREF=\"(.*)\">', text)\n",
    "    titleofFile = re.findall('<BR><TD> (.*)\\n', text)\n",
    "    #print(nameofFile)\n",
    "    #print(titleofFile)\n",
    "\n",
    "    if check == False:\n",
    "        nameofFile = nameofFile[2:]\n",
    "        check = True\n",
    "    file_name.append(nameofFile)\n",
    "    titles.append(titleofFile)    \n",
    "    print(len(nameofFile), len(titleofFile))    \n",
    "        \n",
    "    for allfiles in range(len(nameofFile)):\n",
    "        data.append((str(item) +\"\\\\\"+ str(nameofFile[allfiles]),titleofFile[allfiles]))\n",
    "        \n",
    "print('Total Number of files are:',len(data))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here titles and file_names are in list of lists i.e one list has 452 cells, second has 0 cells and third has 15 cells\n",
    "#Here we are joining the three list onto one\n",
    "\n",
    "titles = [j for i in titles for j in i]\n",
    "file_name = [j for i in file_name for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(titles))\n",
    "len(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Lakshmi Praffulla\\\\Desktop\\\\Information Retrival Assignment 2\\\\Dataset_for_question1\\\\stories\\\\SRE\\\\sre01.txt',\n",
       " 'SRE: The Saga Of The Best SRE Game Ever Played! By Josh Renaud')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data is in the form = [(path of the file, tilte)]\n",
    "data[452]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have 467 files in total\n",
    "total_files = len(data)\n",
    "total_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C:\\\\Users\\\\Lakshmi Praffulla\\\\Desktop\\\\Information Retrival Assignment 2\\\\Dataset_for_question1\\\\stories\\\\13chil.txt', 'The Story of the Sly Fox')\n",
      "FOR CHILDREN:\n",
      "\n",
      "                                   Sly Fox\n",
      "\n",
      "    Mr. Rabbit sat on his front porch rocking, eating a great big carrot, \n",
      "and looking.\n",
      "\n",
      "    \"Looks like Sly Fox coming down the road,\" he said to himself, walking \n",
      "to the end of the porch. Shading his eyes with his paws, he exclaimed, \"It \n",
      "is Sly Fox.\" \n",
      "\n",
      "    \"Good morning Mr. Rabbit,\" cried Sly Fox, as he walked across the yard. \n",
      "\"Good morning,\" replied Mr. Rabbit, a slight frown on his face. \n",
      "\n",
      "    \"Well,\" said Sly Fox, \"as I haven't seen you in so long a time, thought \n",
      "I would stop and chat a while.\" \n",
      "\n",
      "    Mr. Rabbit could not be rude in his own home, even to an enemy, so he \n",
      "offered Sly Fox a seat on the porch. \n",
      "\n",
      "    \"Take a chair,\" he said politely. \tBut Sly Fox did not stay long, and as \n",
      "he was leaving, he asked: \"Mr. Rabbit, my mother is having a good dinner \n",
      "tonight. Won't you, Mrs. Rabbit, and your three little rabs come to dinner \n",
      "with me?\" \n",
      "\n",
      "    Oh, thought Mr. Rabbit, he knows about my little rabs and wants to take \n",
      "us off to eat us. He pretended to be disappointed as he replied: \"Sorry, Sly \n",
      "Fox, we have an engagement for today, but if you want us we can come \n",
      "tomorrow.\" \n",
      "\n",
      "    At this Sly Fox chuckled inwardly, and readily agreed to come for them \n",
      "the next day. Wishing Mr. Rabbit \"Good day\", he trotted on down the road \n",
      "toward his home. \n",
      "\n",
      "    As soon as he was out of sight, Mr. Rabbit ran into his house and called \n",
      "Mrs. Rabbit. \"Get all our things together,\" he said, \"and put rubber boots \n",
      "on our little rabs. We have to move quickly to the Piney Woods across the \n",
      "brook. Old Sly Fox has found our home and will destroy us.\" \n",
      "\n",
      "    In no time at all the Rabbit family had moved, and the little rabs were \n",
      "delighted with their new home. A woodland of towering pines it was, the \n",
      "ground covered with pine needles which made a soft carpeting. The wind made \n",
      "music in the pine trees, birds sang, and the fragrance of flowers filled the \n",
      "air. They found a huge hollow tree where Mr. Rabbit burrowed deep and made \n",
      "them a cozy home. Squirrels had left nuts hidden around in the old tree. \n",
      "Owls hooted throughout the night, crickets chirped merrily. \n",
      "\n",
      "    Next morning old Sly Fox knocked on the door where he had left Mr. \n",
      "Rabbit. Mrs. Hedgehog answered the door. \"Good morning, Mrs. Hedgehog. Is \n",
      "Mr. Rabbit in?\" inquired Sly Fox with a wicked grin and a cunning look in \n",
      "his eyes. \n",
      "\n",
      "    \"No,\" replied Mrs. Eedgehog, none too cordially. \"The Rabbit family \n",
      "moved to parts unknown right after you left yesterday.\" \n",
      "\n",
      "    \"Ah,\" exclaimed old Sly Fox, \"Mr. Rabbit and family were going to have \n",
      "dinner with me. My mother has planned a real feast. Why don't you come and \n",
      "enjoy it with us?\" \n",
      "\n",
      "    \"Oh,\" replied Mrs. Hedgehog, smacking her lips and thinking of all the \n",
      "goodies, \"I have just moved in and there is so much to do! Why not let it go \n",
      "until tomorrow?\" \n",
      "\n",
      "    \"Do you like nice young grasshoppers?\" asked Sly Fox softly. \n",
      "\n",
      "    \"Do I? Nothing so good as tender young grasshoppers,\" answered Mrs. \n",
      "Hedgehog, fairly dribbling at the mouth at the thought of such a dainty. \n",
      "\n",
      "    \"Well,\" said Sly Fox, \"we pass a field where there are any number of \n",
      "them. Come get in this sack, and when I stop in the field we will open the \n",
      "sack and rake in all of them we want. Mother will bake them with apples and \n",
      "they will be deilicious!\" This was too much for greedy Mrs. Hedgehog to \n",
      "resist, so in the sack she went. Sly Fox with a grin grabbed the sack, threw \n",
      "it over his shoulder and trotted toward home. \tAfter going a long way, Mrs. \n",
      "Hedgehog became suspicious and cried, \"How long before we reach that field \n",
      "of grasshoppers?\" \n",
      "\n",
      "    \"Why, you silly, greedy hedgehog, there is no field of grasshoppers for \n",
      "you. I am going to eat you for my dinner. It's you with apple dumplings that \n",
      "my mother will bake.\" \n",
      "\n",
      "    Every hair on Mrs. Hedgehog's head stood on end with fright. Oh, how \n",
      "foolish she had been! Her greed had trapped her. If only she had stayed home \n",
      "and straightened her house and cooked her own dinner, she would not have \n",
      "been in this sack to be eaten by Sly Fox. Greediness never pays, she thought \n",
      "to herself. \n",
      "\n",
      "    Sly Fox became tired, and as a slight rain had begun to fall, he looked \n",
      "for a dry place to sit down. Throwing the sack to the ground and chuckling \n",
      "at the thought of sitting on Mrs. Hedgehog, he dropped heavily upon the \n",
      "sack. \n",
      "\n",
      "    \"Wow, Wow!\" he cried, jumping quickly up, for Mrs. Hedgehog shot her \n",
      "sharp quills into him with all her might. \n",
      "\n",
      "    Sly Fox ran to and fro trying to pull out the quills, but they had gone \n",
      "too deep. Home he ran, screaming to his mother. Old Mother Fox threw him \n",
      "over a log and began pulling out the quills, at the same time calling to a \n",
      "neighbor fox to bring some honey to put on the places where the quills had \n",
      "been. \n",
      "\n",
      "    Mrs. Hedgehog crawled out of the bag and began walking slowly toward \n",
      "home. She thought to herself that never again would she be so greedy and \n",
      "allow herself to be fooled by Sly Fox or any one else. \n",
      "\n",
      "    Meanwhile, Mr. Rabbit and family were living happily in Piney Woods. The \n",
      "little rabs played on the crystal clear brook that ran through the woods, \n",
      "wading, sailing little leaf boats, and trying to catch the silvery minnows \n",
      "darting here and there. \n",
      "\n",
      "    Late one evening Papa and Mama Rabbit were sitting before the cozy fire \n",
      "talking. Papa Rabbit had on his house robe and bedroom slippers, reading the \n",
      "newspaper. Every now and then he looked over his spectacles lovingly at \n",
      "dainty little Mama Rabbit, dressed in a flowered housecoat and red slippers \n",
      "and knitting little socks for the little rabs. \n",
      "\n",
      "    \"Sniff! Sniff! Sniff!\" came suddenly to their ears. \n",
      "\n",
      "    \"Sly Fox!\" whispered Papa Rabbit, his face now full of concern and \n",
      "alarm. \n",
      "\n",
      "    \"Yes,\" agreed Mama Rabbit, her voice trembling with fright. \n",
      "\n",
      "    \"Go cover the little rabs with straw and tell them to be very, very \n",
      "quiet,\" instructed Papa Rabbit. \n",
      "\n",
      "    Mrs. Rabbit quickly covered the little rabs and cautioned them to be as \n",
      "quiet as mice. Since they were well behaved and obedient little rabs, they \n",
      "did just as their mother told them. \n",
      "\n",
      "    \"I left my big stick beside the old oak tree,\" cried Papa Rabbit under \n",
      "his breath. \"What shall we do?\" \n",
      "\n",
      "    \"Sniff! Sniff! Sniff!\" went Sly Fox again, scratching up the earth by \n",
      "the old hollow tree as he began to dig furiously. The poor little Rabbit \n",
      "family sat still and frightened, their hearts thumping, their paws shaking, \n",
      "and their eyes bulging with panic. Suddenly in the distance they heard the \n",
      "\"Toot! Toot!Toot!\" of horns, and the \"Woof! Woof! Woof!\" of barking dogs. \n",
      "\n",
      "    Papa Rabbit whispered, \"Fox hunters!\" as his heart gave a bound of \n",
      "relief. \n",
      "\n",
      "    Nearer and nearer came the baying of the hounds and the music of the \n",
      "horns. Old Sly Fox was so busily digging that he failed to hear at first, \n",
      "but suddenly he stopped digging, and threw back his ears to listen. Then he \n",
      "quickly jumped away from the log where the Rabbit family lived and started \n",
      "running. \n",
      "\n",
      "    But the hounds were right after him, baying loudly with all their might. \n",
      "The horses' feet beat out an excited rhythm as the red-coated fox hunters \n",
      "urged them on in the chase. Up hill, over the meadows they ran. \n",
      "\n",
      "    Sly Fox was now running for his life, but the dogs were getting closer \n",
      "and closer. He jumped across the brook and spied a hole among some bushes. \n",
      "Into this he slid, and as the dogs went down the side of the stream of water \n",
      "before they jumped across they lost his scent. Sly Fox quickly ran out of \n",
      "the hole and took off in the opposite direction from the way the dogs were \n",
      "going. He had been so frightened and so near death that he resolved to \n",
      "himself never to bother the Rabbit family again. \n",
      "\n",
      "    Meanwhile, when Papa Rabbit had heard the hounds start the chase, he \n",
      "turned to Mama Rabbit and cried, \"Safe at last! Call our little rabs for \n",
      "prayers of thanksgiving and praise to our Father which art in heaven.\" \n",
      "\n",
      "    After prayers, Mama Rabbit hustled about making mint tea for her and \n",
      "Papa Rabbit, and hot chocolate piled high with whipped cream for the little \n",
      "rabs. After that time they Lived happily among the great whispering pines, \n",
      "never bothered by old Sly Fox.\n",
      "                                                          --Beulah Murrelle\n"
     ]
    }
   ],
   "source": [
    "def retrive_document_text(doc_id):\n",
    "    print(data[doc_id])\n",
    "    file = open(data[doc_id][0], 'r', encoding='cp1250')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    print(text)\n",
    "    \n",
    "\n",
    "#Documents to read with document_id    \n",
    "retrive_document_text(1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematization(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for i in tokens:\n",
    "        new_text = new_text + \" \" + lemmatizer.lemmatize(i)\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming was not done in this question\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    #data = stemming(data)\n",
    "    data = lematization(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    #data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting of the data into text and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = []\n",
    "processed_title = []\n",
    "\n",
    "for i in data[:total_files]:\n",
    "    file = open(i[0], 'r', encoding=\"utf8\", errors='ignore')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "\n",
    "    processed_text.append(word_tokenize(str(preprocess(text))))\n",
    "    processed_title.append(word_tokenize(str(preprocess(i[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going',\n",
       " 'one',\n",
       " 'hundred',\n",
       " 'west',\n",
       " 'fifty',\n",
       " 'three',\n",
       " 'north',\n",
       " 'jim',\n",
       " 'prentice',\n",
       " 'one',\n",
       " 'thousand',\n",
       " 'nine',\n",
       " 'hundred',\n",
       " 'ninety']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_title[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Coefficient based document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union(list1,list2):\n",
    "    final_list = list(set(list1) | set(list2))\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(list1,list2):\n",
    "    final_list = list(set(list1) & set(list2))\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JaccardsCoefficient(query_list):\n",
    "    JC = {}\n",
    "    doc_id = 0\n",
    "    for item in processed_text:\n",
    "        #item \n",
    "        #query_list\n",
    "    \n",
    "        after_union = union(item,query_list)\n",
    "        after_intersection = intersection(item,query_list)\n",
    "    \n",
    "        JC[doc_id] = len(after_intersection)/len(after_union)\n",
    "    \n",
    "        doc_id = doc_id + 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Sorting the JC decreasing \n",
    "        final_ans = dict(sorted(JC.items(),key=operator.itemgetter(1),reverse=True))\n",
    "            \n",
    "    return final_ans\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(query,k):\n",
    "    query_afterPreprocess =  preprocess(query)\n",
    "    query_list = query_afterPreprocess.split(\" \")\n",
    "    query_list.remove('')\n",
    "    #print(query_list)\n",
    "    after_jc = JaccardsCoefficient(query_list)\n",
    "    #print(after_jc.keys())\n",
    "    count = -1\n",
    "    list_append = []\n",
    "    for item in after_jc.keys():\n",
    "        count += 1\n",
    "        if count == k:\n",
    "            #print('Top ',k,' Documents based on Jaccard score are',list_append)\n",
    "            print('Top ',k,' Documents based on Jaccard score are :')\n",
    "            for i in list_append:\n",
    "                print(file_name[i])\n",
    "#                 print(titles[i]+ \" - \" +file_name[i])\n",
    "            \n",
    "            return\n",
    "        list_append.append(item)\n",
    "    \n",
    "    #print('Top ',k,' Documents based on Jaccard score are',list_append)\n",
    "    print('Top ',k,' Documents based on Jaccard score are :')\n",
    "    \n",
    "    for i in list_append:\n",
    "        print(file_name[i])\n",
    "#         print(titles[i]+\" - \"+file_name[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the String   : 100 west by 53 north\n",
      "Enter how many closest matches required  : 10\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "Top  10  Documents based on Jaccard score are :\n",
      "kneeslapper\n",
      "kneeslapper.txt\n",
      "traitor.txt\n",
      "poem-4.txt\n",
      "write\n",
      "poem-1.txt\n",
      "poem-2.txt\n",
      "mike.txt\n",
      "burltrs\n",
      "snowmaid.txt\n",
      "-------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "getInput = input('Enter the String   : ')\n",
    "k = int(input('Enter how many closest matches required  : '))\n",
    "print('-------------------------------------------------------------------------------------------------------------')\n",
    "execute(getInput,k)\n",
    "print('-------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF_IDF Based Document Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_text has all the text_data of each document stored in a list\n",
    "# processed_text = [[Doc_0 text],[Doc_1 text],.....]\n",
    "\n",
    "#Natural Term Frequency = tf\n",
    "\n",
    "natural_term_frequency = []\n",
    "for item in processed_text:\n",
    "    dictionary_tf ={}\n",
    "    \n",
    "    for word in item:\n",
    "        if dictionary_tf.get(word) == None:\n",
    "            dictionary_tf[word] = 1\n",
    "        else:\n",
    "            count = dictionary_tf.get(word)\n",
    "            count += 1\n",
    "            dictionary_tf[word] = count\n",
    "    \n",
    "    natural_term_frequency.append(dictionary_tf)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural_term_frequency[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithmic Term Frequency = 1 + log(tf)\n",
    "\n",
    "logarithmic_term_frequency = []\n",
    "\n",
    "for item in natural_term_frequency:\n",
    "    dictionary_ltf = {}\n",
    "    allkeys = item.keys()\n",
    "    \n",
    "    for word in allkeys:\n",
    "        natural_freq = item.get(word)\n",
    "        log_freq = 1 + np.log(natural_freq)\n",
    "        dictionary_ltf[word] = log_freq\n",
    "        \n",
    "    logarithmic_term_frequency.append(dictionary_ltf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logarithmic_term_frequency[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmented Term Frequency = 0.5 + ((0.5*tf)/max(tf))\n",
    "\n",
    "\n",
    "augmented_term_frequency = []\n",
    "\n",
    "for item in natural_term_frequency:\n",
    "    dictionary_atf = {}\n",
    "    allkeys = item.keys()\n",
    "    max_tf = max(item.values())\n",
    "    \n",
    "    for word in allkeys:\n",
    "        natural_freq = item.get(word)\n",
    "        aug_freq = 0.5 + ((0.5 * natural_freq)/max_tf)\n",
    "        dictionary_atf[word] = aug_freq\n",
    "    \n",
    "    augmented_term_frequency.append(dictionary_atf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmented_term_frequency[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean Term Frequency\n",
    "#Can't do boolean as we are storing in a dictionary of list  . So 0 will never occur . Calculation will be fine\n",
    "\n",
    "boolean_term_frequency = []\n",
    "\n",
    "for item in natural_term_frequency:\n",
    "    dictionary_btf = {}\n",
    "    allkeys = item.keys()\n",
    "    \n",
    "    for word in allkeys:\n",
    "        natural_freq = item.get(word)\n",
    "        if natural_freq > 0:\n",
    "            bool_freq = 1\n",
    "        else:\n",
    "            bool_freq = 0\n",
    "        dictionary_btf[word] = bool_freq\n",
    "        \n",
    "    boolean_term_frequency.append(dictionary_btf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boolean_term_frequency[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Log average Term Frequency\n",
    "\n",
    "logave_term_frequency = []\n",
    "\n",
    "for item in natural_term_frequency:\n",
    "    \n",
    "    dictionary_latf = {}\n",
    "    allkeys = item.keys()\n",
    "    allvalues = item.values()\n",
    "    avg = sum(allvalues)/len(allvalues)\n",
    "    \n",
    "    for word in allkeys:\n",
    "        natural_freq = item.get(word)\n",
    "        logave = (1+np.log(natural_freq))/(1+np.log(avg)) \n",
    "        dictionary_latf[word] = logave\n",
    "    \n",
    "    logave_term_frequency.append(dictionary_latf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logave_term_frequency[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various variants of Term Frequencies :\n",
    "        #logave_term_frequency\n",
    "        #boolean_term_frequency\n",
    "        #augmented_term_frequency\n",
    "        #logarithmic_term_frequency\n",
    "        #natural_term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Frequency \n",
    "\n",
    "document_frequency = {}\n",
    "\n",
    "for item in processed_text:\n",
    "    \n",
    "    unique_terms = list(set(item))\n",
    "    \n",
    "    for word in unique_terms:\n",
    "        \n",
    "        if document_frequency.get(word) == None:\n",
    "            document_frequency[word] = 1\n",
    "        else:\n",
    "            count = document_frequency.get(word)\n",
    "            count += 1\n",
    "            document_frequency[word] = count\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45095"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_frequency = { term : in how many documents the term is present }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45095\n",
      "Total Number of documents are 467\n"
     ]
    }
   ],
   "source": [
    "print(len(document_frequency))\n",
    "N = len(data)\n",
    "print('Total Number of documents are',N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse Document Frequency \n",
    "\n",
    "inverse_document_frequency = {}\n",
    "\n",
    "keys_df = document_frequency.keys()\n",
    "\n",
    "for item in keys_df:\n",
    "    \n",
    "    getvalue = document_frequency[item]\n",
    "    in_doc_freq = np.log(N/getvalue)\n",
    "    inverse_document_frequency[item] = in_doc_freq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse_document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF weighting \n",
    "#Natural_Term_Frequency\n",
    "\n",
    "\n",
    "\n",
    "list_return = []\n",
    "\n",
    "for item in natural_term_frequency:\n",
    "    dict_inside = {}\n",
    "    allkeys = item.keys()\n",
    "    \n",
    "    for word in allkeys:\n",
    "        current_freq = item[word]\n",
    "        idf_for_word = inverse_document_frequency[word]\n",
    "        update_freq = current_freq * idf_for_word\n",
    "        dict_inside[word] = update_freq\n",
    "    \n",
    "    list_return.append(dict_inside)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_return[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function block for calculating TF-IDF\n",
    "\n",
    "def calculate_tfidf(list_tf):\n",
    "    list_return = []\n",
    "    \n",
    "    for item in list_tf:\n",
    "        dict_inside = {}\n",
    "        allkeys = item.keys()\n",
    "        \n",
    "        for word in allkeys:\n",
    "            current_freq = item[word]\n",
    "            idf_for_word = inverse_document_frequency[word]\n",
    "            update_freq = current_freq * idf_for_word\n",
    "            dict_inside[word] = update_freq\n",
    "    \n",
    "        list_return.append(dict_inside)\n",
    "    \n",
    "    return list_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating TF-IDF for various variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(natural_term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_n = calculate_tfidf(natural_term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_log = calculate_tfidf(logarithmic_term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_aug = calculate_tfidf(augmented_term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_bool = calculate_tfidf(boolean_term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_logave = calculate_tfidf(logave_term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRankdoc(variant_list,query_list):\n",
    "    \n",
    "    doc_rank = {}\n",
    "    for i in range(N):\n",
    "        doc_rank[i] = 0.0\n",
    "        \n",
    "    for query_word in query_list:\n",
    "        count = -1\n",
    "        for item in variant_list:\n",
    "            count += 1\n",
    "            if query_word in item :\n",
    "                score = item[query_word]\n",
    "                doc_rank[count] += score\n",
    "                \n",
    "    doc_rank = dict(sorted(doc_rank.items(),key=operator.itemgetter(1),reverse=True))           \n",
    "    \n",
    "    return doc_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk(list_tfidf,query_list,k,string):\n",
    "    \n",
    "    \n",
    "    list_tf = getRankdoc(list_tfidf,query_list)\n",
    "    count = -1\n",
    "    flag = 0\n",
    "    list_append = []\n",
    "    for item in list_tf.keys():\n",
    "        count += 1\n",
    "        if count == k:\n",
    "            print('Top ',k,' Documents based on ',string,' tf-idf are',list_append)\n",
    "            for i in list_append:\n",
    "                print(file_name[i])\n",
    "#                 print(titles[i])\n",
    "            flag = 1\n",
    "            break\n",
    "        list_append.append(item)\n",
    "    \n",
    "    if flag == 0:\n",
    "        print('Top ',k,' Documents based on ',string,' tf-idf are:',list_append)    \n",
    "        for i in list_append:\n",
    "            print(file_name[i])\n",
    "#             print(titles[i])\n",
    "            \n",
    "    print()        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_two(query,k):\n",
    "    query_afterPreprocess =  preprocess(query)\n",
    "    query_list = query_afterPreprocess.split(\" \")\n",
    "    query_list.remove('')\n",
    "    \n",
    "\n",
    "    topk(tfidf_n,query_list,k,'Natural')\n",
    "    topk(tfidf_log,query_list,k,'Logarithmic')   \n",
    "    topk(tfidf_aug,query_list,k,'Augmented')\n",
    "    topk(tfidf_bool,query_list,k,'Boolean')\n",
    "    topk(tfidf_logave,query_list,k,'Logarithmic Average')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute_two(query,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the String   : 50000 variety of flowers\n",
      "Enter how many closest matches required  : 10\n",
      "\n",
      "Top  10  Documents based on  Natural  tf-idf are [127, 310, 214, 356, 41, 126, 418, 224, 76, 97]\n",
      "dakota.txt\n",
      "outcast.dos\n",
      "gulliver.txt\n",
      "quest\n",
      "archive\n",
      "cybersla.txt\n",
      "timem.hac\n",
      "history5.txt\n",
      "breaks1.asc\n",
      "bureau.txt\n",
      "\n",
      "Top  10  Documents based on  Logarithmic  tf-idf are [76, 418, 356, 310, 214, 226, 359, 127, 126, 378]\n",
      "breaks1.asc\n",
      "timem.hac\n",
      "quest\n",
      "outcast.dos\n",
      "gulliver.txt\n",
      "hitch3.txt\n",
      "radar_ra.txt\n",
      "dakota.txt\n",
      "cybersla.txt\n",
      "sick-kid.txt\n",
      "\n",
      "Top  10  Documents based on  Augmented  tf-idf are [140, 418, 150, 214, 394, 76, 26, 270, 359, 129]\n",
      "domain.poe\n",
      "timem.hac\n",
      "empty.txt\n",
      "gulliver.txt\n",
      "sqzply.txt\n",
      "breaks1.asc\n",
      "aesop11.txt\n",
      "long1-3.txt\n",
      "radar_ra.txt\n",
      "darkness.txt\n",
      "\n",
      "Top  10  Documents based on  Boolean  tf-idf are [26, 76, 78, 129, 140, 145, 150, 214, 226, 270]\n",
      "aesop11.txt\n",
      "breaks1.asc\n",
      "breaks3.asc\n",
      "darkness.txt\n",
      "domain.poe\n",
      "elite.app\n",
      "empty.txt\n",
      "gulliver.txt\n",
      "hitch3.txt\n",
      "long1-3.txt\n",
      "\n",
      "Top  10  Documents based on  Logarithmic Average  tf-idf are [76, 140, 418, 310, 356, 130, 279, 226, 359, 224]\n",
      "breaks1.asc\n",
      "domain.poe\n",
      "timem.hac\n",
      "outcast.dos\n",
      "quest\n",
      "day.in.mcdonald\n",
      "mcdonaldl.txt\n",
      "hitch3.txt\n",
      "radar_ra.txt\n",
      "history5.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getInputtwo = input('Enter the String   : ')\n",
    "k = int(input('Enter how many closest matches required  : '))\n",
    "print()\n",
    "execute_two(getInputtwo,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf based vector space document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Documents as Vectors\n",
    "## This vector contains log tf-idf values . Using this normalize the vector \n",
    "\n",
    "## Then create query as vector of lof tf-idf values. \n",
    "## Normalize the query vector\n",
    "\n",
    "## Then apply cosine rule to each document vector to that of query \n",
    "## Take maximum of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## document_frequency = { term : in how many documents the term is present }\n",
    "## tf-idf_frequency i.e tfidf_log = [  {term : frequency , term : frequency,..} , { term:frequency , term:frequency, .. } ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All terms that are present\n",
    "all_terms = list(document_frequency.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45095"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector = []\n",
    "\n",
    "for item in tfidf_log:\n",
    "    #item is a dictionary. it is a document actually\n",
    "    doc_inside = []\n",
    "    for word in all_terms:\n",
    "        #for each term\n",
    "        \n",
    "        if word in item :\n",
    "            #check if the word is present in the document. if yes then store its frequency or else store 0.0\n",
    "            temp = item[word]\n",
    "            doc_inside.append(temp)\n",
    "        else:\n",
    "            doc_inside.append(0.0)\n",
    "            \n",
    "    doc_vector += [doc_inside]        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosinevalues(query):\n",
    "    \n",
    "    \n",
    "    query_text = word_tokenize(str(preprocess(query)))\n",
    "    \n",
    "    #term_freq_query\n",
    "    term_freq_query = {}\n",
    "\n",
    "    for item in query_text:\n",
    "    \n",
    "        if item in term_freq_query:\n",
    "            temp = term_freq_query[item]\n",
    "            temp += 1\n",
    "            term_freq_query[item] = temp\n",
    "        else:\n",
    "             term_freq_query[item] = 1\n",
    "    \n",
    "    \n",
    "    # Coverting into log frequency\n",
    "\n",
    "    for item in term_freq_query.keys():\n",
    "    \n",
    "        temp = term_freq_query[item]\n",
    "        new = 1 + np.log(temp)\n",
    "        term_freq_query[item] = new\n",
    "    \n",
    "    #inverse_document_frequency\n",
    "    tf_idf_query = {}\n",
    "\n",
    "    for item in term_freq_query.keys():\n",
    "    \n",
    "        if item in inverse_document_frequency :\n",
    "        \n",
    "            temp = inverse_document_frequency[item]\n",
    "            toadd = temp * term_freq_query[item]\n",
    "        \n",
    "            tf_idf_query[item] = toadd\n",
    "        \n",
    "        else:\n",
    "            tf_idf_query[item] = 0.0\n",
    "    \n",
    "    \n",
    "    #  query vector \n",
    "\n",
    "    query_vector = []\n",
    "\n",
    "    for word in all_terms :\n",
    "    \n",
    "        if word in tf_idf_query :\n",
    "            temp = tf_idf_query[word]\n",
    "            query_vector.append(temp)\n",
    "        else:\n",
    "            query_vector.append(0.0)\n",
    "    \n",
    "    \n",
    "    #Normalization of doc vectors\n",
    "    \n",
    "    norm_doc_vector = []\n",
    "\n",
    "    for item in doc_vector:\n",
    "        #each doc list\n",
    "    \n",
    "        norm = 0.0\n",
    "        for freq in item:\n",
    "            norm += (freq*freq)\n",
    "    \n",
    "        norm = 1 / np.sqrt(norm)\n",
    "    \n",
    "        temp_list = []\n",
    "    \n",
    "        for freq in item:\n",
    "            k = freq * norm\n",
    "            temp_list.append(k)\n",
    "        \n",
    "        norm_doc_vector.append(temp_list)\n",
    "        \n",
    "    #Normalization of query vector    \n",
    "    \n",
    "    norm_query_vector = []\n",
    "\n",
    "    normal = 0.0\n",
    "    for item in query_vector:\n",
    "        normal += (item*item)\n",
    "    \n",
    "    normal = 1/np.sqrt(normal)\n",
    "    for item in query_vector:\n",
    "        norm_query_vector.append(item * normal)\n",
    "    \n",
    "    \n",
    "    #Compute Cosine Scores!!!\n",
    "    \n",
    "    cosine_scores = {}\n",
    "\n",
    "    doc_id = -1\n",
    "    for item in norm_doc_vector:\n",
    "    \n",
    "        doc_id += 1\n",
    "        list_doc = np.array(item)\n",
    "        list_query = np.array(norm_query_vector)\n",
    "        score = sum(list(list_doc * list_query))\n",
    "    \n",
    "        cosine_scores[doc_id] = score\n",
    "    \n",
    "    sorted_cosine_scores = dict(sorted(cosine_scores.items(),key=operator.itemgetter(1),reverse=True))\n",
    "    \n",
    "#     print(sorted_cosine_scores)\n",
    "    \n",
    "    #Compute Cosine Score with giving weightage to titles!!!\n",
    "    \n",
    "    weighted_cosine_score = cosine_scores\n",
    "    \n",
    "    query_weight = remove_stop_words(query)\n",
    "    query_weight = query_weight.split(\" \")\n",
    "    \n",
    "    for i in query_weight :\n",
    "        doc = 0\n",
    "        \n",
    "        for title in titles:\n",
    "            \n",
    "            if i.lower() in title.lower():\n",
    "                s = cosine_scores[doc]\n",
    "                ns = s * 1.4\n",
    "                weighted_cosine_score[doc] = ns\n",
    "            doc += 1\n",
    "#     print(\"-------------------\")\n",
    "    weighted_cosine_score = dict(sorted(weighted_cosine_score.items(),key=operator.itemgetter(1),reverse=True))\n",
    "#     print(weighted_cosine_score)\n",
    "    \n",
    "    return sorted_cosine_scores,weighted_cosine_score    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_three(getInputthree,k):\n",
    "    \n",
    "    cosine_value,cosine_value_weighted = cosinevalues(getInputthree)\n",
    "    string = \"\"\n",
    "    count = -1\n",
    "    flag = 0\n",
    "    list_append = []\n",
    "    print(\"Not Giving spl attention to document titles\")\n",
    "    for item in cosine_value.keys():\n",
    "        count += 1\n",
    "        if count == k:\n",
    "            print('Top ',k,' Documents based on ',string,' tf-idf are',list_append)\n",
    "            for i in list_append:\n",
    "                print(file_name[i]+ \" ||| \"+ titles[i])\n",
    "#                 print(titles[i]+ \" - \"+ file_name[i])\n",
    "            flag = 1\n",
    "            break\n",
    "        list_append.append(item)\n",
    "    \n",
    "    if flag == 0:\n",
    "        print('Top ',k,' Documents based on ',string,' tf-idf are:',list_append)    \n",
    "        for i in list_append:\n",
    "            print(file_name[i]+\" ||| \"+ titles[i])\n",
    "#             print(titles[i] + \" - \"+ file_name[i])\n",
    "\n",
    "            \n",
    "    print()\n",
    "    \n",
    "    print(\"Giving spl attention to document titles\")\n",
    "    string = \"\"\n",
    "    count = -1\n",
    "    flag = 0\n",
    "    list_append = []\n",
    "    for item in cosine_value_weighted.keys():\n",
    "        count += 1\n",
    "        if count == k:\n",
    "            print('Top ',k,' Documents based on ',string,' tf-idf are',list_append)\n",
    "            for i in list_append:\n",
    "                print(file_name[i]+\" ||| \"+ titles[i])\n",
    "#                 print(titles[i]+ \" - \"+ file_name[i])\n",
    "            flag = 1\n",
    "            break\n",
    "        list_append.append(item)\n",
    "    \n",
    "    if flag == 0:\n",
    "        print('Top ',k,' Documents based on ',string,' tf-idf are:',list_append)    \n",
    "        for i in list_append:\n",
    "            print(file_name[i]+\" ||| \"+ titles[i])\n",
    "#             print(titles[i] + \" - \"+ file_name[i])\n",
    "\n",
    "            \n",
    "    print()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the String   : 100 west by 53 north\n",
      "Enter how many closest matches required  : 10\n",
      "\n",
      "Not Giving spl attention to document titles\n",
      "Top  10  Documents based on    tf-idf are [317, 0, 106, 42, 384, 105, 79, 390, 328, 154]\n",
      "peace.fun ||| Vision of Peace on Earth\n",
      "100west.txt ||| Going 100 West by 53 North by Jim Prentice (1990)\n",
      "candle.hum ||| The Wind, The Cold, The Candle, by Walter Hawn\n",
      "arctic.txt ||| The Arctic Circle and Beyond, by Jim Prentice (1990)\n",
      "snowmaid.txt ||| The Fable of the Snow Maiden\n",
      "campfire.txt ||| The Best Campire Story of the Week: Do you have a B.C.?\n",
      "bruce-p.txt ||| The Adventure of the Bruce-Partington Plans\n",
      "spam.key ||| The Key and I, or, My Travels in the Sky, and What I Found There, by Padre Martini, Archdukebishop of West Texas\n",
      "pphamlin.txt ||| The Pied Piper of Hamelin\n",
      "enginer.txt ||| The Adventure of the Engineer's Thumb\n",
      "\n",
      "Giving spl attention to document titles\n",
      "Top  10  Documents based on    tf-idf are [0, 317, 106, 390, 42, 384, 127, 105, 79, 328]\n",
      "100west.txt ||| Going 100 West by 53 North by Jim Prentice (1990)\n",
      "peace.fun ||| Vision of Peace on Earth\n",
      "candle.hum ||| The Wind, The Cold, The Candle, by Walter Hawn\n",
      "spam.key ||| The Key and I, or, My Travels in the Sky, and What I Found There, by Padre Martini, Archdukebishop of West Texas\n",
      "arctic.txt ||| The Arctic Circle and Beyond, by Jim Prentice (1990)\n",
      "snowmaid.txt ||| The Fable of the Snow Maiden\n",
      "dakota.txt ||| Rifts World Book: North Dakota by Sean Satterlee\n",
      "campfire.txt ||| The Best Campire Story of the Week: Do you have a B.C.?\n",
      "bruce-p.txt ||| The Adventure of the Bruce-Partington Plans\n",
      "pphamlin.txt ||| The Pied Piper of Hamelin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getInputthree = input('Enter the String   : ')\n",
    "k = int(input('Enter how many closest matches required  : '))\n",
    "print()\n",
    "execute_three(getInputthree,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name[169]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Going 100 West by 53 North by Jim Prentice (1990)'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrive_document_text(53)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Done with document Vector!!\n",
    "## Have to normalize !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"danah away from the veiw \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = word_tokenize(str(preprocess(q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danah', 'away', 'veiw']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#term_freq_query\n",
    "\n",
    "term_freq_query = {}\n",
    "\n",
    "for item in query_text:\n",
    "    \n",
    "    if item in term_freq_query:\n",
    "        temp = term_freq_query[item]\n",
    "        temp += 1\n",
    "        term_freq_query[item] = temp\n",
    "    else:\n",
    "         term_freq_query[item] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'danah': 1, 'away': 1, 'veiw': 1}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freq_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coverting into log frequency\n",
    "\n",
    "for item in term_freq_query.keys():\n",
    "    \n",
    "    temp = term_freq_query[item]\n",
    "    new = 1 + np.log(temp)\n",
    "    term_freq_query[item] = new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'danah': 1.0, 'away': 1.0, 'veiw': 1.0}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " term_freq_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate td-idf .  Already tf done . Have to do multiply with idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse_document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse_document_frequency\n",
    "tf_idf_query = {}\n",
    "\n",
    "for item in term_freq_query.keys():\n",
    "    \n",
    "    if item in inverse_document_frequency :\n",
    "        \n",
    "        temp = inverse_document_frequency[item]\n",
    "        toadd = temp * term_freq_query[item]\n",
    "        \n",
    "        tf_idf_query[item] = toadd\n",
    "        \n",
    "    else:\n",
    "        tf_idf_query[item] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'danah': 6.1463292576688975, 'away': 0.3874274837916167, 'veiw': 0.0}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45095"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    " #query vector \n",
    "\n",
    "query_vector = []\n",
    "\n",
    "for word in all_terms :\n",
    "    \n",
    "    if word in tf_idf_query :\n",
    "        temp = tf_idf_query[word]\n",
    "        query_vector.append(temp)\n",
    "    else:\n",
    "        query_vector.append(0.0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of doc_vector\n",
    "\n",
    "norm_doc_vector = []\n",
    "\n",
    "for item in doc_vector:\n",
    "    #each doc list\n",
    "    \n",
    "    norm = 0.0\n",
    "    for freq in item:\n",
    "        norm += (freq*freq)\n",
    "    \n",
    "    norm = 1 / np.sqrt(norm)\n",
    "    \n",
    "    temp_list = []\n",
    "    \n",
    "    for freq in item:\n",
    "        k = freq * norm\n",
    "        temp_list.append(k)\n",
    "        \n",
    "    norm_doc_vector.append(temp_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_doc_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of query_vector\n",
    "\n",
    "norm_query_vector = []\n",
    "\n",
    "normal = 0.0\n",
    "for item in query_vector:\n",
    "    normal += (item*item)\n",
    "    \n",
    "normal = 1/np.sqrt(normal)\n",
    "for item in query_vector:\n",
    "    norm_query_vector.append(item * normal)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have :\n",
    "#           norm_doc_vector\n",
    "#           norm_query_vector\n",
    "\n",
    "# Calculate cosine scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores = {}\n",
    "\n",
    "doc_id = -1\n",
    "for item in norm_doc_vector:\n",
    "    \n",
    "    doc_id += 1\n",
    "    list_doc = np.array(item)\n",
    "    list_query = np.array(norm_query_vector)\n",
    "    score = sum(list(list_doc * list_query))\n",
    "    \n",
    "    cosine_scores[doc_id] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cosine_scores = dict(sorted(cosine_scores.items(),key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cosine = sorted_cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Adventure of the Three Gables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = remove_stop_words(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Adventure Three Gables'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "qu = q.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adventure', 'Three', 'Gables']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Adventure of the Three Gables :: Adventure ::  8  ::  0.0005185780265533383  ::  0.0007260092371746736\n",
      "The Adventure of the Three Students :: Adventure ::  11  ::  0.0003979648294628428  ::  0.0005571507612479799\n",
      "The Adventure of the Six Napoleons :: Adventure ::  16  ::  0.00042172553883332986  ::  0.0005904157543666618\n",
      "The Adventure of the Abbey Grange (RP Story) :: Adventure ::  20  ::  0.00042924989269658114  ::  0.0006009498497752136\n",
      "The Adventures of Aladdin :: Adventure ::  23  ::  0.0008434323901176068  ::  0.0011808053461646495\n",
      "Sayed's Adventures :: Adventure ::  24  ::  0.0006080563602399  ::  0.00085127890433586\n",
      "The Adventures of Tom Thumb :: Adventure ::  25  ::  0.0007926736347653065  ::  0.001109743088671429\n",
      "The Adventures of Bert and Bernece, by  Francis U. Kaltenbaugh :: Adventure ::  51  ::  0.0006041219398619397  ::  0.0008457707158067155\n",
      "The Adventure of Black Peter :: Adventure ::  61  ::  0.0002665698833653131  ::  0.00037319783671143833\n",
      "Breaks: The Adventures of Richard Nixon in the 21st Century, by Philip H. Farber (1992-1993) :: Adventure ::  76  ::  0.00033827198217361096  ::  0.0004735807750430553\n",
      "Breaks: The Adventures of Richard Nixon in the 21st Century, by Philip H. Farber (1992-1993) :: Adventure ::  77  ::  0.00030180363131254175  ::  0.0004225250838375584\n",
      "Breaks: The Adventures of Richard Nixon in the 21st Century, by Philip H. Farber (1992-1993) :: Adventure ::  78  ::  0.000374893039823003  ::  0.0005248502557522042\n",
      "The Adventure of the Bruce-Partington Plans :: Adventure ::  79  ::  0.00042848109809686557  ::  0.0005998735373356118\n",
      "They Don't Come Any Larger: The Continuing Adventures of Mr. X. :: Adventure ::  87  ::  0.0004359301344594162  ::  0.0006103021882431826\n",
      "The Adventures of Cap'n Roger (Part 2) :: Adventure ::  91  ::  0.0  ::  0.0\n",
      "The Adventure of the Empty House :: Adventure ::  150  ::  0.00025413242033849346  ::  0.0003557853884738908\n",
      "The Adventure of the Engineer's Thumb :: Adventure ::  154  ::  0.0005438729142838792  ::  0.0007614220799974309\n",
      "The Adventure of the Golden Pince-Nez :: Adventure ::  207  ::  0.0004753991030888897  ::  0.0006655587443244455\n",
      "The First Adventures of IDI :: Adventure ::  238  ::  0.0003358001175845688  ::  0.00047012016461839626\n",
      "The Adventure of the Lion's Mane :: Adventure ::  265  ::  0.000462648206073056  ::  0.0006477074885022783\n",
      "The Adventure of the Mazarin Stone :: Adventure ::  278  ::  0.0004464875559437578  ::  0.0006250825783212609\n",
      "The Adventure of the Missing Three-Quarter (A story) :: Adventure ::  284  ::  0.00040937442538784973  ::  0.0005731241955429896\n",
      "The Adventure of Shoscombe Old Place :: Adventure ::  375  ::  0.00058580465872486  ::  0.000820126522214804\n",
      "The Adventure of the Solitary Cyclist :: Adventure ::  388  ::  0.00041186374671735906  ::  0.0005766092454043026\n",
      "The Adventure of the Veiled Lodger :: Adventure ::  434  ::  0.0005220332700520858  ::  0.0007308465780729201\n",
      "The Adventure of Wisteria Lodge :: Adventure ::  441  ::  0.00035177780053948567  ::  0.0004924889207552799\n",
      "The Adventure of the Three Gables :: Three ::  8  ::  0.0005185780265533383  ::  0.0007260092371746736\n",
      "The Adventure of the Three Students :: Three ::  11  ::  0.0003979648294628428  ::  0.0005571507612479799\n",
      "The Story of the Three Wishes :: Three ::  12  ::  0.0005612472161976634  ::  0.0007857461026767287\n",
      "Three Paragraphs: Excertps from Something :: Three ::  86  ::  0.001141488167412403  ::  0.001598083434377364\n",
      "The Emperor's Three Questions by Leo Tolstoy :: Three ::  147  ::  0.0  ::  0.0\n",
      "The Story of Goldilocks and the Three Bears :: Three ::  205  ::  0.0011074099113986392  ::  0.0015503738759580948\n",
      "The Adventure of the Missing Three-Quarter (A story) :: Three ::  284  ::  0.00040937442538784973  ::  0.0005731241955429896\n",
      "The Three Golden Rules of Poetry by Michael Howard :: Three ::  448  ::  0.0  ::  0.0\n",
      "The Adventure of the Three Gables :: Gables ::  8  ::  0.0005185780265533383  ::  0.0007260092371746736\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for item in qu:\n",
    "    doc = 0\n",
    "    \n",
    "    for title in titles:\n",
    "        \n",
    "        if item.lower() in title.lower():\n",
    "            score = temp_cosine[doc]\n",
    "            new_score = score * 1.4\n",
    "            print(title + \" :: \" + item+\" :: \",doc,\" :: \",score,\" :: \",new_score)\n",
    "        \n",
    "        doc += 1\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Emperor's Three Questions by Leo Tolstoy\""
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[147]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
