{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Lakshmi\n",
      "[nltk_data]     Praffulla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Lemmatizations\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Lakshmi\n",
      "[nltk_data]     Praffulla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Opening the files\n",
    "\n",
    "directory_r = [f for f in os.listdir('./20_newsgroups') if not f.startswith('.')]\n",
    "directory_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorable_reviews = []\n",
    "with open('file.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "for i in lines:\n",
    "    i = i.replace(\"\\n\",\"\")\n",
    "    k = i.split(\" \")\n",
    "    favorable_reviews.append(int(k[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(favorable_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematization(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for i in tokens:\n",
    "        new_text = new_text + \" \" + lemmatizer.lemmatize(i)\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) \n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    \n",
    "    data = lematization(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_text = []\n",
    "path_id = []\n",
    "final_path = []\n",
    "co = 0\n",
    "\n",
    "for i in range(len(directory_r)):\n",
    "    files = os.listdir('./20_newsgroups/' + directory_r[i])\n",
    "    for j in range(len(files)):\n",
    "        path = './20_newsgroups/' + directory_r[i] + '/' + files[j]\n",
    "        doc_id = files[j]\n",
    "        path_id.append(doc_id)\n",
    "        final_path.append(path)\n",
    "        text = open(path,'r',errors='ignore').read()\n",
    "        \n",
    "        processed_text.append(word_tokenize(str(preprocess(text))))\n",
    "        \n",
    "        \n",
    "        if co % 500 == 0:\n",
    "            print(co)\n",
    "        co += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_text has all the text_data of each document stored in a list\n",
    "# processed_text = [[Doc_0 text],[Doc_1 text],.....]\n",
    "\n",
    "#Natural Term Frequency = tf\n",
    "\n",
    "natural_term_frequency = []\n",
    "for item in processed_text:\n",
    "    dictionary_tf ={}\n",
    "    \n",
    "    for word in item:\n",
    "        if dictionary_tf.get(word) == None:\n",
    "            dictionary_tf[word] = 1\n",
    "        else:\n",
    "            count = dictionary_tf.get(word)\n",
    "            count += 1\n",
    "            dictionary_tf[word] = count\n",
    "    \n",
    "    natural_term_frequency.append(dictionary_tf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithmic Term Frequency = 1 + log(tf)\n",
    "\n",
    "logarithmic_term_frequency = []\n",
    "\n",
    "for item in natural_term_frequency:\n",
    "    dictionary_ltf = {}\n",
    "    allkeys = item.keys()\n",
    "    \n",
    "    for word in allkeys:\n",
    "        natural_freq = item.get(word)\n",
    "        log_freq = 1 + np.log(natural_freq)\n",
    "        dictionary_ltf[word] = log_freq\n",
    "        \n",
    "    logarithmic_term_frequency.append(dictionary_ltf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logarithmic_term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Frequency \n",
    "\n",
    "document_frequency = {}\n",
    "\n",
    "for item in processed_text:\n",
    "    \n",
    "    unique_terms = list(set(item))\n",
    "    \n",
    "    for word in unique_terms:\n",
    "        \n",
    "        if document_frequency.get(word) == None:\n",
    "            document_frequency[word] = 1\n",
    "        else:\n",
    "            count = document_frequency.get(word)\n",
    "            count += 1\n",
    "            document_frequency[word] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164428"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164428\n",
      "Total Number of documents are 19997\n"
     ]
    }
   ],
   "source": [
    "print(len(document_frequency))\n",
    "N = len(processed_text)\n",
    "print('Total Number of documents are',N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse Document Frequency \n",
    "\n",
    "inverse_document_frequency = {}\n",
    "\n",
    "keys_df = document_frequency.keys()\n",
    "\n",
    "for item in keys_df:\n",
    "    \n",
    "    getvalue = document_frequency[item]\n",
    "    in_doc_freq = np.log(N/getvalue)\n",
    "    inverse_document_frequency[item] = in_doc_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF weighting \n",
    "#Natural_Term_Frequency\n",
    "\n",
    "list_return = []\n",
    "\n",
    "for item in natural_term_frequency:\n",
    "    dict_inside = {}\n",
    "    allkeys = item.keys()\n",
    "    \n",
    "    for word in allkeys:\n",
    "        current_freq = item[word]\n",
    "        idf_for_word = inverse_document_frequency[word]\n",
    "        update_freq = current_freq * idf_for_word\n",
    "        dict_inside[word] = update_freq\n",
    "    \n",
    "    list_return.append(dict_inside)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function block for calculating TF-IDF\n",
    "\n",
    "\n",
    "def calculate_tfidf(list_tf):\n",
    "    list_return = []\n",
    "    co = 0\n",
    "    for item in list_tf:\n",
    "        dict_inside = {}\n",
    "        allkeys = item.keys()\n",
    "        \n",
    "        for word in allkeys:\n",
    "            current_freq = item[word]\n",
    "            idf_for_word = inverse_document_frequency[word]\n",
    "            update_freq = current_freq * idf_for_word\n",
    "            update_freq = update_freq + favorable_reviews[co]\n",
    "            dict_inside[word] = update_freq\n",
    "    \n",
    "        list_return.append(dict_inside)\n",
    "        co += 1\n",
    "    return list_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logarithmic_term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_log = calculate_tfidf(logarithmic_term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164428"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "co = 0\n",
    "for word in document_frequency:\n",
    "    dict_toappend = {}\n",
    "    index = 0\n",
    "    for doc in tfidf_log:\n",
    "        if word in doc:\n",
    "            dict_toappend[path_id[index]] = doc[word]\n",
    "        index += 1    \n",
    "    inverted_index[word] = dict_toappend\n",
    "    if co % 5000 == 0:\n",
    "        print(co)\n",
    "    co += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_inverted_index = {} \n",
    "for item in inverted_index:\n",
    "    getdict = inverted_index[item]\n",
    "    getdictappend = dict(sorted(getdict.items(),key=operator.itemgetter(1),reverse=True))\n",
    "    sorted_inverted_index[item] = getdictappend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "campionlist_word = {}\n",
    "for item in sorted_inverted_index:\n",
    "    campionlist_word[item] = list(sorted_inverted_index[item].keys())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# campionlist_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will choose \"R\", which is the size of the chanpion list\n",
    "## we will break them into two list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlow_camp = {} \n",
    "for item in campionlist_word: \n",
    "    getlist = campionlist_word[item] \n",
    "    highlow_camp[item] = [getlist[:r],getlist[r:]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['105563']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highlow_camp['canyon'][0][:1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getqueryvector(query):\n",
    "      \n",
    "    query_text = word_tokenize(str(preprocess(query)))\n",
    "    \n",
    "    #term_freq_query\n",
    "    term_freq_query = {}\n",
    "\n",
    "    for item in query_text:\n",
    "    \n",
    "        if item in term_freq_query:\n",
    "            temp = term_freq_query[item]\n",
    "            temp += 1\n",
    "            term_freq_query[item] = temp\n",
    "        else:\n",
    "             term_freq_query[item] = 1\n",
    "    \n",
    "    \n",
    "    # Coverting into log frequency\n",
    "\n",
    "    for item in term_freq_query.keys():\n",
    "    \n",
    "        temp = term_freq_query[item]\n",
    "        new = 1 + np.log(temp)\n",
    "        term_freq_query[item] = new\n",
    "    \n",
    "    #inverse_document_frequency\n",
    "    tf_idf_query = {}\n",
    "\n",
    "    for item in term_freq_query.keys():\n",
    "    \n",
    "        if item in inverse_document_frequency :\n",
    "        \n",
    "            temp = inverse_document_frequency[item]\n",
    "            toadd = temp * term_freq_query[item]\n",
    "        \n",
    "            tf_idf_query[item] = toadd\n",
    "        \n",
    "        else:\n",
    "            tf_idf_query[item] = 0.0\n",
    "    \n",
    "    \n",
    "    #  query vector \n",
    "\n",
    "    query_vector = []\n",
    "#     list(document_frequency.keys())\n",
    "    for word in query_text:\n",
    "    \n",
    "        if word in tf_idf_query :\n",
    "            temp = tf_idf_query[word]\n",
    "            query_vector.append(temp)\n",
    "        else:\n",
    "            query_vector.append(0.0)\n",
    "            \n",
    "    #Normalization of query vector    \n",
    "    \n",
    "    norm_query_vector = []\n",
    "\n",
    "    normal = 0.0\n",
    "    for item in query_vector:\n",
    "        normal += (item*item)\n",
    "    \n",
    "    normal = 1/np.sqrt(normal)\n",
    "    for item in query_vector:\n",
    "        norm_query_vector.append(item * normal)\n",
    "    \n",
    "    \n",
    "    return norm_query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "query = \"\"\n",
    "query_text = word_tokenize(str(preprocess(query)))\n",
    "queryvector = getqueryvector(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_eval = []\n",
    "if k < r:\n",
    "    # we will implement high list  \n",
    "    for item in highlow_camp:\n",
    "        documents_eval += highlow_camp[item][0][:k]    \n",
    "else:\n",
    "    # we should also consider low list\n",
    "    extra = r - k\n",
    "    for item in highlow_camp:\n",
    "        documents_eval += highlow_camp[item][0]\n",
    "        documents_eval += highlow_camp[item][1][:extra]\n",
    "        \n",
    "documents_eval = list(set(documents_eval))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16299"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each document we have to find out vector\n",
    "\n",
    "document_vector = []\n",
    "\n",
    "for eachdoc in documents_eval:\n",
    "    getplace = path_id.index(eachdoc)\n",
    "    getdict = tfidf_log[getplace]\n",
    "    \n",
    "    inside_list = []\n",
    "    \n",
    "    for word in query_text:\n",
    "        if word in getdict:\n",
    "            temp = getdict[word]\n",
    "            inside_list.append(temp)\n",
    "        else:\n",
    "            inside_list.append(0.0)\n",
    "    document_vector.append(inside_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize the document vector\n",
    "\n",
    "norm_doc_vector = []\n",
    "\n",
    "for item in document_vector:\n",
    "    \n",
    "    norm = 0.0\n",
    "    for freq in item:\n",
    "        norm += (freq*freq)\n",
    "        \n",
    "    if np.sqrt(norm) == 0.0 :\n",
    "        norm == 0.0\n",
    "    else:\n",
    "        norm = 1/np.sqrt(norm)\n",
    "    \n",
    "    temp_list = []\n",
    "    \n",
    "    for freq in item :\n",
    "        k = freq * norm\n",
    "        temp_list.append(k)\n",
    "    norm_doc_vector.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores = {}\n",
    "\n",
    "doc_id = -1\n",
    "for item in norm_doc_vector:\n",
    "    \n",
    "    doc_id += 1\n",
    "    list_doc = np.array(item)\n",
    "    list_query = np.array(queryvector)\n",
    "    score = sum(list(list_doc * list_query))\n",
    "    \n",
    "    cosine_scores[doc_id] = score\n",
    "    \n",
    "sorted_cosine_scores = dict(sorted(cosine_scores.items(),key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top  10 Documenst for the query is: [4314, 10571, 8244, 1246, 7973, 13692, 1156, 12981, 12968, 8574]\n",
      "./20_newsgroups/comp.sys.mac.hardware/51673\n",
      "./20_newsgroups/rec.sport.hockey/53971\n",
      "./20_newsgroups/rec.motorcycles/104393\n",
      "./20_newsgroups/comp.graphics/38407\n",
      "./20_newsgroups/rec.autos/103771\n",
      "./20_newsgroups/sci.med/59332\n",
      "./20_newsgroups/comp.graphics/38317\n",
      "./20_newsgroups/sci.electronics/54485\n",
      "./20_newsgroups/sci.electronics/54351\n",
      "./20_newsgroups/rec.motorcycles/104724\n"
     ]
    }
   ],
   "source": [
    "anslist = list(sorted_cosine_scores.keys())[:k]\n",
    "print(\"The top \",k,\"Documenst for the query is:\",anslist)\n",
    "print\n",
    "for i in anslist:\n",
    "    print(final_path[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
