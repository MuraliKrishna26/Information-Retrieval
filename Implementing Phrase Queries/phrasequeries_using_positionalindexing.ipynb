{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Lakshmi\n",
      "[nltk_data]     Praffulla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Lakshmi\n",
      "[nltk_data]     Praffulla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Lemmatizations\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Opening the files\n",
    "\n",
    "directory_r = [f for f in os.listdir('./20_newsgroups') if not f.startswith('.')]\n",
    "directory_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.graphics', 'rec.motorcycles']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = []\n",
    "for in_die in directory_r :\n",
    "    \n",
    "    if in_die == 'comp.graphics' or in_die == 'rec.motorcycles' :\n",
    "        directory += [in_die]\n",
    "        \n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Lakshmi\n",
      "[nltk_data]     Praffulla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Opening the text file\n",
    "## Created inverted index where the keys are not sorted \n",
    "## Value for the key is a posting list which is also not sorted\n",
    "\n",
    "vocab = {}\n",
    "words = []\n",
    "for i in range(len(directory)):\n",
    "    files = os.listdir('./20_newsgroups/' + directory[i])\n",
    "    \n",
    "    for j in range(len(files)):\n",
    "        \n",
    "        path = './20_newsgroups/' + directory[i] + '/' + files[j]\n",
    "        \n",
    "        doc_id = files[j]\n",
    "        \n",
    "        word_position = 0\n",
    "        words = []\n",
    "        \n",
    "        text = open(path,'r',errors='ignore').read()\n",
    "        \n",
    "#         for p in \"!.,:@#$%^&?<>*()[}{]-=;/\\\"\\\\\\t\\n\":\n",
    "#             if p in '\\n;?:!.,.':\n",
    "#                 text = text.replace(p,' ')\n",
    "#             else:\n",
    "#                 text = text.replace(p,'')\n",
    "        \n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        #digits are removed....\n",
    "#         for k in tokens:\n",
    "#             if k.isdigit():\n",
    "#                 tokens.remove(k)\n",
    "        \n",
    "        for item in tokens:\n",
    "            item = item.lower()\n",
    "            after_lemmatization = lemmatizer.lemmatize(item)\n",
    "            if 'a'=='a': \n",
    "#              if after_lemmatization not in stop_words :\n",
    "#                  words += [after_lemmatization]  \n",
    "                each_word = after_lemmatization                \n",
    "                vocab_keys = vocab.keys()\n",
    "                \n",
    "                if each_word in vocab_keys :\n",
    "                    \n",
    "                    dictionary_inside = vocab[each_word]\n",
    "                    dictionary_inside_keys = dictionary_inside.keys()\n",
    "                    \n",
    "                    if doc_id in dictionary_inside_keys:\n",
    "                        \n",
    "                        dictionary_inside_list = dictionary_inside[doc_id]\n",
    "                        dictionary_inside_list.append(word_position)\n",
    "                        \n",
    "                        vocab[each_word][doc_id] = dictionary_inside_list\n",
    "                    \n",
    "                    else:\n",
    "                        dictionary_inside_list = []\n",
    "                        dictionary_inside_list.append(word_position)\n",
    "                        \n",
    "                        vocab[each_word][doc_id] = dictionary_inside_list\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    inside_dictionary = {}\n",
    "                    \n",
    "                    inside_list = []\n",
    "                    inside_list.append(word_position)\n",
    "                    \n",
    "                    inside_dictionary[doc_id] = inside_list\n",
    "                    \n",
    "                    vocab[each_word] = inside_dictionary\n",
    "            \n",
    "                word_position = word_position + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37528"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Length of the vocab!!!\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------------------------------------- Query -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"University Of Alabama At Birmingham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_lemmat = []\n",
    "for item in tokens:\n",
    "    item = item.lower()\n",
    "    phrase_lemmat += [lemmatizer.lemmatize(item)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['university', 'of', 'alabama', 'at', 'birmingham']\n"
     ]
    }
   ],
   "source": [
    "print(phrase_lemmat)\n",
    "\n",
    "result = {}\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in phrase_lemmat :\n",
    "    \n",
    "    inside_dictionary = vocab[item]\n",
    "    \n",
    "    if len(result) == 0 :\n",
    "        result = inside_dictionary\n",
    "    else:\n",
    "        doc_list = list(set(result.keys()) & set(inside_dictionary.keys()))\n",
    "        temp_dic = {}\n",
    "        \n",
    "        for doc in doc_list :\n",
    "            temp_dic[doc] = result[doc]    \n",
    "        result = {}    \n",
    "        result = temp_dic\n",
    "        if len(doc_list) == 0:\n",
    "            print(\"No documents\")\n",
    "            break\n",
    "        \n",
    "        for doc in doc_list :\n",
    "            \n",
    "            list_posting = result[doc]\n",
    "            temp_pos = []\n",
    "            \n",
    "            for pos in list_posting:\n",
    "                \n",
    "                if pos+count in inside_dictionary[doc]:\n",
    "                    temp_pos.append(pos)\n",
    "            \n",
    "            result[doc] = temp_pos\n",
    "            \n",
    "            if len(result[doc]) == 0 :\n",
    "                del result[doc]\n",
    "        \n",
    "    count = count + 1    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'38462': [56, 1265],\n",
       " '37946': [72, 139],\n",
       " '37948': [45, 462],\n",
       " '37923': [62, 208]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents retrived are : 4\n"
     ]
    }
   ],
   "source": [
    "# Length of documents retrived\n",
    "\n",
    "print(\"Number of Documents retrived are :\" ,len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Documents retrived are : {'38462': [56, 1265], '37946': [72, 139], '37948': [45, 462], '37923': [62, 208]}\n"
     ]
    }
   ],
   "source": [
    "## The Documents retrived with the query\n",
    "\n",
    "print(\"List of Documents retrived are :\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------- End -----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
