{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Lakshmi\n",
      "[nltk_data]     Praffulla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Lakshmi\n",
      "[nltk_data]     Praffulla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Lemmatizations\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Opening the files\n",
    "\n",
    "directory = [f for f in os.listdir('./20_newsgroups') if not f.startswith('.')]\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Stemming (Not Used)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our own list of some block words to be avoided  \n",
    " \n",
    "block_words = ['newsgroups', 'xref', 'path', 'from', 'subject', 'sender', 'organisation', 'apr','gmt', 'last','better','never','every','even','two','good','used','first','need','going','must','really','might','well','without','made','give','look','try','far','less','seem','new','make','many','way','since','using','take','help','thanks','send','free','may','see','much','want','find','would','one','like','get','use','also','could','say','us','go','please','said','set','got','sure','come','lot','seems','able','anything','put', '--', '|>', '>>', '93', 'xref', 'cantaloupe.srv.cs.cmu.edu', '20', '16', \"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\", '21', '19', '10', '17', '24', 'reply-to:', 'thu', 'nntp-posting-host:', 're:','25''18'\"i'd\"'>i''22''fri,''23''>the','references:','xref:','sender:','writes:','1993','organization:']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Lakshmi\n",
      "[nltk_data]     Praffulla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(block_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Opening the text file\n",
    "## Created inverted index where the keys are not sorted \n",
    "## Value for the key is a posting list which is also not sorted\n",
    "\n",
    "vocab = {}\n",
    "for i in range(len(directory)):\n",
    "    files = os.listdir('./20_newsgroups/' + directory[i])\n",
    "    for j in range(len(files)):\n",
    "        path = './20_newsgroups/' + directory[i] + '/' + files[j]\n",
    "        doc_id = files[j]\n",
    "        text = open(path,'r',errors='ignore').read()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        for k in tokens:\n",
    "            if k.isdigit():\n",
    "                tokens.remove(k)\n",
    "                \n",
    "        \n",
    "        \n",
    "        for item in tokens:\n",
    "            item = item.lower()\n",
    "            after_lemmatization = lemmatizer.lemmatize(item)\n",
    "            if after_lemmatization not in stop_words and after_lemmatization not in block_words :\n",
    "                if len(after_lemmatization) > 2:\n",
    "                    \n",
    "                    if vocab.get(after_lemmatization) != None :\n",
    "                        get_list = vocab.get(after_lemmatization)\n",
    "                        if doc_id not in get_list :\n",
    "                            vocab[after_lemmatization] += [doc_id]\n",
    "                    else:\n",
    "                        vocab[after_lemmatization] = [doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293194"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sorting the posting lists \n",
    "## And appended the posting list to the dictonary vocab.\n",
    "\n",
    "keys_all = vocab.keys()\n",
    "\n",
    "for element in keys_all:\n",
    "    list_to_sort = vocab.get(element)\n",
    "    list_to_append = [int(i) for i in list_to_sort]\n",
    "    list_to_append.sort() \n",
    "    vocab[element] = list_to_append\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All documents in one list \n",
    "\n",
    "documentlist = []\n",
    " \n",
    "\n",
    "for i in range(len(directory)):\n",
    "    x = os.listdir('./20_newsgroups/' + directory[i])\n",
    "    for j in x:\n",
    "        k = int(j)\n",
    "        documentlist.append(k)  \n",
    "\n",
    "documentlist.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AND operation between two tokens\n",
    "\n",
    "def andOperation(tokenone,tokentwo,count):\n",
    "    listone = vocab.get(tokenone)\n",
    "    listtwo = vocab.get(tokentwo)\n",
    "    resultantlist = []\n",
    "\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "\n",
    "    while p1<len(listone) and p2<len(listtwo) :\n",
    "        if listone[p1] == listtwo[p2]:\n",
    "            count = count + 1\n",
    "            resultantlist.append(listone[p1])\n",
    "            p1 = p1 + 1\n",
    "            p2 = p2 + 1\n",
    "        elif listone[p1] > listtwo[p2]:\n",
    "            count = count + 1\n",
    "            p2 = p2 + 1\n",
    "        else:\n",
    "            count = count + 1\n",
    "            p1 = p1 + 1  \n",
    "\n",
    "    return resultantlist,count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AND operation between two lists\n",
    "## Here Tokenone and Tokentwo are list's\n",
    "\n",
    "def andOperationlist(tokenone,tokentwo,count):\n",
    "    listone = tokenone\n",
    "    listtwo = tokentwo\n",
    "    resultantlist = []\n",
    "\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "\n",
    "    while p1<len(listone) and p2<len(listtwo) :\n",
    "        if listone[p1] == listtwo[p2]:\n",
    "            count = count + 1\n",
    "            resultantlist.append(listone[p1])\n",
    "            p1 = p1 + 1\n",
    "            p2 = p2 + 1\n",
    "        elif listone[p1] > listtwo[p2]:\n",
    "            count = count + 1\n",
    "            p2 = p2 + 1\n",
    "        else:\n",
    "            count = count + 1\n",
    "            p1 = p1 + 1  \n",
    "\n",
    "    return resultantlist,count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OR operation between two lists\n",
    "\n",
    "\n",
    "def orOperation(tokenone,tokentwo,count):\n",
    "    \n",
    "    if len(tokentwo) >= len(tokentwo) :\n",
    "        listone = vocab.get(tokentwo)\n",
    "        listtwo = vocab.get(tokenone)\n",
    "    else :\n",
    "        listone = vocab.get(tokenone)\n",
    "        listtwo = vocab.get(tokentwo)\n",
    "    \n",
    "    \n",
    "    resultantlist = []\n",
    "\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "\n",
    "    while p1<len(listone) and p2<len(listtwo) :\n",
    "        if listone[p1] == listtwo[p2]:\n",
    "            count = count + 1\n",
    "            resultantlist.append(listone[p1])\n",
    "            p1 = p1 + 1\n",
    "            p2 = p2 + 1\n",
    "        elif listone[p1] > listtwo[p2]:\n",
    "            count = count + 1\n",
    "            resultantlist.append(listtwo[p2])\n",
    "            p2 = p2 + 1\n",
    "        else:\n",
    "            count = count + 1\n",
    "            resultantlist.append(listone[p1])\n",
    "            p1 = p1 + 1\n",
    "\n",
    "    while p1 < len(listone) :\n",
    "        resultantlist.append(listone[p1])\n",
    "        p1 = p1 + 1    \n",
    "\n",
    "    while p2 < len(listtwo) :\n",
    "        resultantlist.append(listtwo[p2])\n",
    "        p2 = p2 + 1\n",
    "\n",
    "    return resultantlist,count  \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OR operation between two lists\n",
    "## Here Tokenone and Tokentwo are list's\n",
    "\n",
    "def orOperationlist(tokenone,tokentwo,count):\n",
    "    \n",
    "    if len(tokentwo) >= len(tokentwo) :\n",
    "        listone = tokentwo\n",
    "        listtwo = tokenone\n",
    "    else :\n",
    "        listone = tokenone\n",
    "        listtwo = tokentwo\n",
    "    \n",
    "    \n",
    "    \n",
    "    resultantlist = []\n",
    "\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "\n",
    "    while p1<len(listone) and p2<len(listtwo) :\n",
    "        if listone[p1] == listtwo[p2]:\n",
    "            count = count + 1\n",
    "            resultantlist.append(listone[p1])\n",
    "            p1 = p1 + 1\n",
    "            p2 = p2 + 1\n",
    "        elif listone[p1] > listtwo[p2]:\n",
    "            count = count + 1\n",
    "            resultantlist.append(listtwo[p2])\n",
    "            p2 = p2 + 1\n",
    "        else:\n",
    "            count = count + 1\n",
    "            resultantlist.append(listone[p1])\n",
    "            p1 = p1 + 1\n",
    "\n",
    "    while p1 < len(listone) :\n",
    "        resultantlist.append(listone[p1])\n",
    "        p1 = p1 + 1    \n",
    "\n",
    "    while p2 < len(listtwo) :\n",
    "        resultantlist.append(listtwo[p2])\n",
    "        p2 = p2 + 1\n",
    "\n",
    "    return resultantlist,count  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT Operations\n",
    "## documentlist has the complete list\n",
    "\n",
    "def notOperation(tokenone,count):\n",
    "    resultantlist = []\n",
    "    listone = vocab.get(tokenone)\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "  \n",
    "    while p1 < len(listone) :\n",
    "    \n",
    "        if listone[p1] == documentlist[p2]:\n",
    "            count = count + 1\n",
    "            #do nothing\n",
    "            p2 = p2 + 1\n",
    "            p1 = p1 + 1\n",
    "        elif listone[p1] > documentlist[p2]:\n",
    "            count = count + 1\n",
    "            resultantlist.append(documentlist[p2])\n",
    "            p2 = p2 + 1\n",
    "  \n",
    "    while p2 < len(documentlist) :\n",
    "        resultantlist.append(documentlist[p2])\n",
    "        p2 = p2 + 1\n",
    "\n",
    "    return resultantlist,count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------FinalQueryWithFunction-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query to be searched \n",
    "\n",
    "Query = \"state-of-the-art and visualization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Query.split(\" \")\n",
    "form_st = []\n",
    "for k in l:\n",
    "    form_st += [lemmatizer.lemmatize(k)] \n",
    "form_st = \" \".join(form_st)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'state-of-the-art and visualization'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "form_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = form_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.split(' or ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_sep = []\n",
    "for or_sep in final:\n",
    "    and_sep += [or_sep.split(' and ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "inside = []\n",
    "while i < len(and_sep) :\n",
    "    \n",
    "    inside = and_sep[i]\n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    while j < len(inside) :\n",
    "        \n",
    "        if len(inside[j].split(\" \")) == 2:\n",
    "                  inside[j],count = notOperation(inside[j].split(\" \")[1],count)  \n",
    "        \n",
    "        if type(inside[j]) == str :\n",
    "            inside[j] = vocab[inside[j]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        j= j + 1\n",
    "    \n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "che = and_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[15183,\n",
       "   15188,\n",
       "   15193,\n",
       "   15219,\n",
       "   15253,\n",
       "   15290,\n",
       "   15429,\n",
       "   16118,\n",
       "   37919,\n",
       "   37920,\n",
       "   38692,\n",
       "   38848,\n",
       "   58052,\n",
       "   58053,\n",
       "   60698,\n",
       "   61001,\n",
       "   61009,\n",
       "   61034,\n",
       "   61135,\n",
       "   84302,\n",
       "   84332,\n",
       "   84414,\n",
       "   84511,\n",
       "   178713],\n",
       "  [8514,\n",
       "   37261,\n",
       "   37915,\n",
       "   37919,\n",
       "   37920,\n",
       "   38257,\n",
       "   38333,\n",
       "   38375,\n",
       "   38376,\n",
       "   38377,\n",
       "   38450,\n",
       "   38460,\n",
       "   38582,\n",
       "   38609,\n",
       "   38658,\n",
       "   38665,\n",
       "   38782,\n",
       "   38818,\n",
       "   38848,\n",
       "   38851,\n",
       "   38852,\n",
       "   38853,\n",
       "   38913,\n",
       "   38942,\n",
       "   39038,\n",
       "   39073,\n",
       "   51779,\n",
       "   54237,\n",
       "   58052,\n",
       "   58053,\n",
       "   58910,\n",
       "   61141,\n",
       "   66433,\n",
       "   67498,\n",
       "   68305]]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(che,key = len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_do = []\n",
    "\n",
    "\n",
    "i = 0 \n",
    "\n",
    "while i < len(che):\n",
    "    and_do = che[i]\n",
    "\n",
    "    if len(and_do) == 1 : \n",
    "        or_do += and_do\n",
    "    else:\n",
    "        j = 0\n",
    "        temp = []\n",
    "        while j < len(and_do) :\n",
    "            \n",
    "            if j == 0 :\n",
    "                temp = and_do[j]\n",
    "            else:\n",
    "                temp,count = andOperationlist(temp,and_do[j],count)\n",
    "            \n",
    "            j = j + 1\n",
    "    \n",
    "        or_do +=[temp]\n",
    "\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = or_do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 0\n",
    "temp = []\n",
    "while i < len(num) :\n",
    "    \n",
    "    if i == 0:\n",
    "        temp = num[i]\n",
    "    else:\n",
    "        temp,count = orOperationlist(temp,num[i],count)\n",
    "        \n",
    "    i = i + 1    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37919, 37920, 38848, 58052, 58053]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents retrived are : 5\n",
      "Minimum Number of total comparisions done : 49\n"
     ]
    }
   ],
   "source": [
    "## 1st Question \n",
    "\n",
    "print(\"Number of Documents retrived are :\" ,len(temp))\n",
    "\n",
    "print(\"Minimum Number of total comparisions done :\" , count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Documents retrived are : [37919, 37920, 38848, 58052, 58053]\n"
     ]
    }
   ],
   "source": [
    "## The Documents retrived with the query\n",
    "\n",
    "print(\"List of Documents retrived are :\",temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------- END OF FIRST QUESTION  ------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
